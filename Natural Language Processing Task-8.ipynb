{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8384739a",
   "metadata": {},
   "source": [
    "### Given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b250e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "5571489\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"English.csv\")\n",
    "text = data_file.read()\n",
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150bd17",
   "metadata": {},
   "source": [
    "### Importing important paclages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf041053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aravi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809cc68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: translate in d:\\users\\aravi\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: requests in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from translate) (2.27.1)\n",
      "Requirement already satisfied: click in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from translate) (8.0.4)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: lxml in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from translate) (4.8.0)\n",
      "Requirement already satisfied: colorama in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from click->translate) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from requests->translate) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from requests->translate) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from requests->translate) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\aravi\\anaconda3\\lib\\site-packages (from requests->translate) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5853c0ab",
   "metadata": {},
   "source": [
    "## Task-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb88d9",
   "metadata": {},
   "source": [
    "You need to translate each word or sentence from English to Spanish, French and German \n",
    "\n",
    "-->cleaning the dataset\n",
    "\n",
    "1)sentence tokenization\n",
    "2)word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c4fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eef992f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    s.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748034d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English words/sentences\\nHi.', 'Run!', 'Run!', 'Who?', 'Wow!', 'Fire!', 'Help!', 'Jump.', 'Stop!', 'Stop!'] 177156\n"
     ]
    }
   ],
   "source": [
    "print(s[:10],len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8edc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "for sentence in sentences: # Lopping through every sentence in the given dataset\n",
    "    words = nltk.word_tokenize(sentence) # Here, we are extracting the important words in the sentences\n",
    "    w.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "788eb6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['English', 'words/sentences', 'Hi', '.'], ['Run', '!'], ['Run', '!'], ['Who', '?'], ['Wow', '!']] 177156\n"
     ]
    }
   ],
   "source": [
    "print(w[:5],len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068faba",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7631178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "#os.listdir('/root/nltk_data/corpora/stopwords/')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93c7d9",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aa0cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "p = []\n",
    "for i in s:\n",
    "    i = i.lower()\n",
    "    P = tokenizer.tokenize(i)\n",
    "    p.extend((P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e20dfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177156, 1145709, ['english', 'words'], 14138)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = list(set(p))\n",
    "len(s),len(p),p[:2],len(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "465e2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cirrhosis', 'left', 'desiring', 'inherently', 'procrastinating'] 13989\n"
     ]
    }
   ],
   "source": [
    "no_punct = []\n",
    "for i in p1:\n",
    "    if i not in stop_words:\n",
    "        no_punct.append(i)\n",
    "print(no_punct[:5],len(no_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf3170",
   "metadata": {},
   "source": [
    "## Stemming And Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4ffa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemma = []\n",
    "def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    l = lemmatizer.lemmatize(word, pos)\n",
    "    lemma.append(l)\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "for i in no_punct:\n",
    "    compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = i, pos = wordnet.VERB)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc81811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cirrhosis',\n",
       "  'leave',\n",
       "  'desire',\n",
       "  'inherently',\n",
       "  'procrastinate',\n",
       "  'addict',\n",
       "  'bellboy',\n",
       "  'brew',\n",
       "  'tipsy',\n",
       "  'taxpayers'],\n",
       " 13989)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma[:10], len(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e0abd",
   "metadata": {},
   "source": [
    "### No Punctuation Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0223c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i dont drink white wine very often', 'we must get together for a drink some time', 'unlock the door', 'i know some students in that school', 'i dont know if it will rain tomorrow or not', 'tom dreams of becoming a millionaire', 'can a case be made for lateterm abortions', 'thank you very much for your invitation', 'he attended the party yesterday', 'i love my new class']\n",
      "123635\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence_no_punct = []\n",
    "\n",
    "for w in set(s):\n",
    "    text = w\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(w+:\\/\\/\\S+)|^rt|http.+?\",\"\", text)\n",
    "    sentence_no_punct.append(text.lower())\n",
    "    \n",
    "print(sentence_no_punct[:10])\n",
    "print(len(sentence_no_punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efffac6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123336, 123635)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sentence_no_punct)), len(sentence_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e12dbd",
   "metadata": {},
   "source": [
    "### Translating from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5a3f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [set(sentence_no_punct)]\n",
    "s = []\n",
    "for i in S:\n",
    "    for j in i:\n",
    "        s.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd2adec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr..french\n",
    "from translate import Translator\n",
    "fr = []\n",
    "for i in s[:50]:\n",
    "    w = i\n",
    "    translator = Translator(to_lang=\"fr\")\n",
    "    translation = translator.translate(w)\n",
    "    fr.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a70ddf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:who has come\n",
      "French Sentence:Qui est venu\n",
      "\n",
      "English Sentence:tom was the one who taught me how to sing country music\n",
      "French Sentence:C’est Tom qui m’a appris à chanter de la musique country\n",
      "\n",
      "English Sentence:are you planning on staying in boston for a long time\n",
      "French Sentence:Avez-vous l’intention de rester à Boston pendant une longue période\n",
      "\n",
      "English Sentence:i know that he keeps his promise\n",
      "French Sentence:Je sais qu'il tient sa parole.\n",
      "\n",
      "English Sentence:i visited my grandmothers house\n",
      "French Sentence:J’ai visité la maison de mes grands-mères\n",
      "\n",
      "English Sentence:we cant trust anyone now\n",
      "French Sentence:Nous ne pouvons faire confiance à personne maintenant\n",
      "\n",
      "English Sentence:i think its somewhere around here\n",
      "French Sentence:Je pense que c’est quelque part par ici\n",
      "\n",
      "English Sentence:this tshirt is too small for me\n",
      "French Sentence:Ce t-shirt est trop petit pour moi\n",
      "\n",
      "English Sentence:this is the true story\n",
      "French Sentence:Voilà, certes, le récit véridique.\n",
      "\n",
      "English Sentence:may i take pictures here\n",
      "French Sentence:Ai-je le droit de prendre des photos, ici ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(s[:10], fr[:10]):\n",
    "    print(f\"English Sentence:{a}\")\n",
    "    print(f\"French Sentence:{b}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb87ae3",
   "metadata": {},
   "source": [
    "### Translating from English to Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6b70412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spanish..es\n",
    "from translate import Translator\n",
    "es=[]\n",
    "for i in s[:50]:\n",
    "    translator= Translator(to_lang=\"es\")\n",
    "    w = i\n",
    "    translation = translator.translate(w)\n",
    "    es.append(translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9259c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:who has come\n",
      "Spanish Sentence:¿Quién ha venido?\n",
      "\n",
      "English Sentence:tom was the one who taught me how to sing country music\n",
      "Spanish Sentence:Tom fue quien me enseñó a cantar música country\n",
      "\n",
      "English Sentence:are you planning on staying in boston for a long time\n",
      "Spanish Sentence:¿Estás planeando quedarte en Boston por mucho tiempo?\n",
      "\n",
      "English Sentence:i know that he keeps his promise\n",
      "Spanish Sentence:Sé que cumple su promesa\n",
      "\n",
      "English Sentence:i visited my grandmothers house\n",
      "Spanish Sentence:Visité la casa de mi abuela\n",
      "\n",
      "English Sentence:we cant trust anyone now\n",
      "Spanish Sentence:No podemos confiar en nadie ahora\n",
      "\n",
      "English Sentence:i think its somewhere around here\n",
      "Spanish Sentence:Creo que está en algún lugar por aquí\n",
      "\n",
      "English Sentence:this tshirt is too small for me\n",
      "Spanish Sentence:Esta camiseta es demasiado pequeña para mí\n",
      "\n",
      "English Sentence:this is the true story\n",
      "Spanish Sentence:Ésta es la exposición auténtica.\n",
      "\n",
      "English Sentence:may i take pictures here\n",
      "Spanish Sentence:¿Puedo sacar fotos aquí?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(s[:10], es[:10]):\n",
    "    print(f\"English Sentence:{a}\")\n",
    "    print(f\"Spanish Sentence:{b}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11dd10d",
   "metadata": {},
   "source": [
    "### Translating from English to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4ad1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#german..de\n",
    "from translate import Translator\n",
    "de = []\n",
    "for i in s[:50]:\n",
    "    translator= Translator(to_lang=\"de\")\n",
    "    w = i\n",
    "    translation = translator.translate(w)\n",
    "    de.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2391343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:who has come\n",
      "German Sentence:Wer ist gekommen\n",
      "\n",
      "English Sentence:tom was the one who taught me how to sing country music\n",
      "German Sentence:Tom war derjenige, der mir beigebracht hat, wie man Country-Musik singt\n",
      "\n",
      "English Sentence:are you planning on staying in boston for a long time\n",
      "German Sentence:Planen Sie, längere Zeit in Boston zu bleiben?\n",
      "\n",
      "English Sentence:i know that he keeps his promise\n",
      "German Sentence:Ich weiß, dass er sein Versprechen hält\n",
      "\n",
      "English Sentence:i visited my grandmothers house\n",
      "German Sentence:Ich besuchte das Haus meiner Großmutter\n",
      "\n",
      "English Sentence:we cant trust anyone now\n",
      "German Sentence:Wir können jetzt niemandem vertrauen\n",
      "\n",
      "English Sentence:i think its somewhere around here\n",
      "German Sentence:Ich denke, es ist irgendwo hier\n",
      "\n",
      "English Sentence:this tshirt is too small for me\n",
      "German Sentence:Dieses T-Shirt ist mir zu klein\n",
      "\n",
      "English Sentence:this is the true story\n",
      "German Sentence:Das ist die wahre Geschichte.\n",
      "\n",
      "English Sentence:may i take pictures here\n",
      "German Sentence:Darf man hier fotografieren?\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for a,b in zip(s[:10], de[:10]):\n",
    "    print(f\"English Sentence:{a}\")\n",
    "    print(f\"German Sentence:{b}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e481c",
   "metadata": {},
   "source": [
    "### Task-2\n",
    "Create a program that needs to automatically correct that spelling from the word or a given sentence. (Language : English) \n",
    "\n",
    "Note: show 100 data point outputs both in words and sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b09b31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "words=[]\n",
    "def compare_stemmer_and_lemmatizer(lemmatizer, word, pos):\n",
    "    l=lemmatizer.lemmatize(word, pos)\n",
    "    words.append(l)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in no_punct:\n",
    "    compare_stemmer_and_lemmatizer(lemmatizer, word = i, pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5dc5535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cirrhosis',\n",
       " 'leave',\n",
       " 'desire',\n",
       " 'inherently',\n",
       " 'procrastinate',\n",
       " 'addict',\n",
       " 'bellboy',\n",
       " 'brew',\n",
       " 'tipsy',\n",
       " 'taxpayers',\n",
       " 'smile',\n",
       " 'employee',\n",
       " 'attachments',\n",
       " 'youâ',\n",
       " 'barter',\n",
       " 'fax',\n",
       " 'undergo',\n",
       " 'audiobooks',\n",
       " 'prod',\n",
       " 'shall',\n",
       " 'rumor',\n",
       " 'rat',\n",
       " 'professional',\n",
       " 'racism',\n",
       " 'bmi',\n",
       " 'zulu',\n",
       " 'informative',\n",
       " 'romantic',\n",
       " 'class',\n",
       " 'advance',\n",
       " 'surrender',\n",
       " 'scanner',\n",
       " 'priest',\n",
       " 'authoritative',\n",
       " 'postman',\n",
       " 'shop',\n",
       " 'sue',\n",
       " 'similarities',\n",
       " 'wombats',\n",
       " 'treat',\n",
       " 'promotion',\n",
       " 'party',\n",
       " 'skeleton',\n",
       " 'dimension',\n",
       " 'purse',\n",
       " 'catalog',\n",
       " 'opponents',\n",
       " 'shortcomings',\n",
       " 'arrange',\n",
       " 'style',\n",
       " 'friction',\n",
       " 'bible',\n",
       " 'growl',\n",
       " 'shape',\n",
       " 'handstand',\n",
       " 'greet',\n",
       " 'still',\n",
       " 'switch',\n",
       " 'apricot',\n",
       " 'orchids',\n",
       " 'mole',\n",
       " 'widow',\n",
       " 'goodness',\n",
       " 'outline',\n",
       " 'control',\n",
       " 'screwdriver',\n",
       " 'giggle',\n",
       " 'hazel',\n",
       " 'antidote',\n",
       " 'ssh',\n",
       " 'bookkeeping',\n",
       " 'smell',\n",
       " 'frankly',\n",
       " 'bowl',\n",
       " 'tire',\n",
       " 'hypocrisy',\n",
       " 'spare',\n",
       " 'remotely',\n",
       " 'birdwatch',\n",
       " 'chemistry',\n",
       " 'injuries',\n",
       " 'motivate',\n",
       " 'generations',\n",
       " 'cranberry',\n",
       " 'evict',\n",
       " 'rome',\n",
       " 'burglaries',\n",
       " 'ship',\n",
       " 'bend',\n",
       " 'train',\n",
       " 'like',\n",
       " 'optimism',\n",
       " 'hospitality',\n",
       " 'turn',\n",
       " 'explore',\n",
       " 'vision',\n",
       " 'anthropologist',\n",
       " 'kin',\n",
       " 'airbag',\n",
       " 'vienna',\n",
       " 'explode']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a23a35",
   "metadata": {},
   "source": [
    "### Task-3\n",
    "\n",
    "Create an application that should be used by the HR Team to filter the resume based on the Skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a71ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ac58503",
   "metadata": {},
   "source": [
    "### Task-4\n",
    "Create a chatbot for Hotel Management to Book Rooms \n",
    "\n",
    "Details collected from : Customer Name, Mobile Number, Address, ID proof, and Room Type and date of arrival and departure date. Keep some eligibility to Book the Room . All through voice or text Note: You can use Rule Based Chatbot or NLP Based Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e7bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9527eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6bb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e254da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cf9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
